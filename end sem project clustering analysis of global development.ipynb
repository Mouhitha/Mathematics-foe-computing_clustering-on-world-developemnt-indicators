{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ac9fbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Birth Rate</th>\n",
       "      <th>Business Tax Rate</th>\n",
       "      <th>CO2 Emissions</th>\n",
       "      <th>Country</th>\n",
       "      <th>Days to Start Business</th>\n",
       "      <th>Ease of Business</th>\n",
       "      <th>Energy Usage</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Health Exp % GDP</th>\n",
       "      <th>Health Exp/Capita</th>\n",
       "      <th>...</th>\n",
       "      <th>Life Expectancy Male</th>\n",
       "      <th>Mobile Phone Usage</th>\n",
       "      <th>Number of Records</th>\n",
       "      <th>Population 0-14</th>\n",
       "      <th>Population 15-64</th>\n",
       "      <th>Population 65+</th>\n",
       "      <th>Population Total</th>\n",
       "      <th>Population Urban</th>\n",
       "      <th>Tourism Inbound</th>\n",
       "      <th>Tourism Outbound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87931.0</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26998.0</td>\n",
       "      <td>54790058957</td>\n",
       "      <td>0.035</td>\n",
       "      <td>$60</td>\n",
       "      <td>...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.039</td>\n",
       "      <td>31719449</td>\n",
       "      <td>0.599</td>\n",
       "      <td>$102,000,000</td>\n",
       "      <td>$193,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9542.0</td>\n",
       "      <td>Angola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7499.0</td>\n",
       "      <td>$9,129,594,819</td>\n",
       "      <td>0.034</td>\n",
       "      <td>$22</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.025</td>\n",
       "      <td>13924930</td>\n",
       "      <td>0.324</td>\n",
       "      <td>$34,000,000</td>\n",
       "      <td>$146,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1617.0</td>\n",
       "      <td>Benin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>$2,359,122,303</td>\n",
       "      <td>0.043</td>\n",
       "      <td>$15</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.029</td>\n",
       "      <td>6949366</td>\n",
       "      <td>0.383</td>\n",
       "      <td>$77,000,000</td>\n",
       "      <td>$50,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>Botswana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>$5,788,311,645</td>\n",
       "      <td>0.047</td>\n",
       "      <td>$152</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1755375</td>\n",
       "      <td>0.532</td>\n",
       "      <td>$227,000,000</td>\n",
       "      <td>$209,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>Burkina Faso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$2,610,959,139</td>\n",
       "      <td>0.051</td>\n",
       "      <td>$12</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.028</td>\n",
       "      <td>11607944</td>\n",
       "      <td>0.178</td>\n",
       "      <td>$23,000,000</td>\n",
       "      <td>$30,000,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Birth Rate Business Tax Rate  CO2 Emissions       Country  \\\n",
       "0       0.020               NaN        87931.0       Algeria   \n",
       "1       0.050               NaN         9542.0        Angola   \n",
       "2       0.043               NaN         1617.0         Benin   \n",
       "3       0.027               NaN         4276.0      Botswana   \n",
       "4       0.046               NaN         1041.0  Burkina Faso   \n",
       "\n",
       "   Days to Start Business  Ease of Business  Energy Usage             GDP  \\\n",
       "0                     NaN               NaN       26998.0     54790058957   \n",
       "1                     NaN               NaN        7499.0  $9,129,594,819   \n",
       "2                     NaN               NaN        1983.0  $2,359,122,303   \n",
       "3                     NaN               NaN        1836.0  $5,788,311,645   \n",
       "4                     NaN               NaN           NaN  $2,610,959,139   \n",
       "\n",
       "   Health Exp % GDP Health Exp/Capita  ...  Life Expectancy Male  \\\n",
       "0             0.035               $60  ...                  67.0   \n",
       "1             0.034               $22  ...                  44.0   \n",
       "2             0.043               $15  ...                  53.0   \n",
       "3             0.047              $152  ...                  49.0   \n",
       "4             0.051               $12  ...                  49.0   \n",
       "\n",
       "   Mobile Phone Usage  Number of Records  Population 0-14  Population 15-64  \\\n",
       "0                 0.0                  1            0.342             0.619   \n",
       "1                 0.0                  1            0.476             0.499   \n",
       "2                 0.0                  1            0.454             0.517   \n",
       "3                 0.1                  1            0.383             0.587   \n",
       "4                 0.0                  1            0.468             0.505   \n",
       "\n",
       "   Population 65+  Population Total  Population Urban  Tourism Inbound  \\\n",
       "0           0.039          31719449             0.599     $102,000,000   \n",
       "1           0.025          13924930             0.324      $34,000,000   \n",
       "2           0.029           6949366             0.383      $77,000,000   \n",
       "3           0.029           1755375             0.532     $227,000,000   \n",
       "4           0.028          11607944             0.178      $23,000,000   \n",
       "\n",
       "   Tourism Outbound  \n",
       "0      $193,000,000  \n",
       "1      $146,000,000  \n",
       "2       $50,000,000  \n",
       "3      $209,000,000  \n",
       "4       $30,000,000  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_org = pd.read_excel(r\"C:\\Users\\91630\\OneDrive\\Desktop\\mcs4\\ens sem project\\World_development_measurement.xlsx\")\n",
    "\n",
    "data_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f573854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized Countries:\n",
      "['Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Costa Rica', 'Croatia', 'Cuba', 'Cyprus', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Fiji', 'Finland', 'France', 'Gabon', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Rwanda', 'Samoa', 'San Marino', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Sweden', 'Switzerland', 'Tajikistan', 'Tanzania', 'Thailand', 'Timor-Leste', 'Togo', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Vietnam', 'Zambia', 'Zimbabwe']\n",
      "195\n",
      "164\n",
      "\n",
      "Territories:\n",
      "['American Samoa', 'Aruba', 'Bahamas, The', 'Bermuda', 'Brunei Darussalam', 'Cayman Islands', 'Congo, Dem. Rep.', 'Congo, Rep.', \"Cote d'Ivoire\", 'Curacao', 'Czech Republic', 'Egypt, Arab Rep.', 'Faeroe Islands', 'French Polynesia', 'Gambia, The', 'Greenland', 'Guam', 'Hong Kong SAR, China', 'Iran, Islamic Rep.', 'Isle of Man', 'Korea, Dem. Rep.', 'Korea, Rep.', 'Kosovo', 'Kyrgyz Republic', 'Lao PDR', 'Macao SAR, China', 'Macedonia, FYR', 'Micronesia, Fed. Sts.', 'New Caledonia', 'Puerto Rico', 'Russian Federation', 'Sao Tome and Principe', 'Sint Maarten (Dutch part)', 'Slovak Republic', 'St. Kitts and Nevis', 'St. Lucia', 'St. Martin (French part)', 'St. Vincent and the Grenadines', 'Swaziland', 'Syrian Arab Republic', 'Turks and Caicos Islands', 'Venezuela, RB', 'Virgin Islands (U.S.)', 'Yemen, Rep.']\n",
      "44\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "recognized_countries = [\n",
    "    'India', 'China', 'United States', 'Indonesia', 'Pakistan', 'Nigeria', 'Brazil', 'Bangladesh',\n",
    "    'Russia', 'Mexico', 'Ethiopia', 'Japan', 'Philippines', 'Egypt', 'DR Congo', 'Vietnam', 'Iran',\n",
    "    'Turkey', 'Germany', 'Thailand', 'United Kingdom', 'Tanzania', 'France', 'South Africa', 'Italy',\n",
    "    'Kenya', 'Myanmar', 'Colombia', 'South Korea', 'Uganda', 'Sudan', 'Spain', 'Argentina', 'Algeria',\n",
    "    'Iraq', 'Afghanistan', 'Poland', 'Canada', 'Morocco', 'Saudi Arabia', 'Ukraine', 'Angola', 'Uzbekistan',\n",
    "    'Yemen', 'Peru', 'Malaysia', 'Ghana', 'Mozambique', 'Nepal', 'Madagascar', 'Côte d\\'Ivoire', 'Venezuela',\n",
    "    'Cameroon', 'Niger', 'Australia', 'North Korea', 'Mali', 'Burkina Faso', 'Syria', 'Sri Lanka', 'Malawi',\n",
    "    'Zambia', 'Romania', 'Chile', 'Kazakhstan', 'Chad', 'Ecuador', 'Somalia', 'Guatemala', 'Senegal', 'Netherlands',\n",
    "    'Cambodia', 'Zimbabwe', 'Guinea', 'Rwanda', 'Benin', 'Burundi', 'Tunisia', 'Bolivia', 'Haiti', 'Belgium', 'Jordan',\n",
    "    'Dominican Republic', 'Cuba', 'South Sudan', 'Sweden', 'Honduras', 'Czech Republic (Czechia)', 'Azerbaijan', 'Greece',\n",
    "    'Papua New Guinea', 'Portugal', 'Hungary', 'Tajikistan', 'United Arab Emirates', 'Belarus', 'Israel', 'Togo', 'Austria',\n",
    "    'Switzerland', 'Sierra Leone', 'Laos', 'Serbia', 'Nicaragua', 'Libya', 'Paraguay', 'Kyrgyzstan', 'Bulgaria', 'Turkmenistan',\n",
    "    'El Salvador', 'Congo', 'Singapore', 'Denmark', 'Slovakia', 'Central African Republic', 'Finland', 'Norway', 'Liberia',\n",
    "    'State of Palestine', 'Lebanon', 'New Zealand', 'Costa Rica', 'Ireland', 'Mauritania', 'Oman', 'Panama', 'Kuwait', 'Croatia',\n",
    "    'Eritrea', 'Georgia', 'Mongolia', 'Moldova', 'Uruguay', 'Bosnia and Herzegovina', 'Albania', 'Jamaica', 'Armenia', 'Gambia',\n",
    "    'Lithuania', 'Qatar', 'Botswana', 'Namibia', 'Gabon', 'Lesotho', 'Guinea-Bissau', 'Slovenia', 'North Macedonia', 'Latvia',\n",
    "    'Equatorial Guinea', 'Trinidad and Tobago', 'Bahrain', 'Timor-Leste', 'Estonia', 'Mauritius', 'Cyprus', 'Eswatini',\n",
    "    'Djibouti', 'Fiji', 'Comoros', 'Guyana', 'Bhutan', 'Solomon Islands', 'Luxembourg', 'Montenegro', 'Suriname', 'Cabo Verde',\n",
    "    'Micronesia', 'Malta', 'Maldives', 'Brunei', 'Bahamas', 'Belize', 'Iceland', 'Vanuatu', 'Barbados', 'Sao Tome & Principe',\n",
    "    'Samoa', 'Saint Lucia', 'Kiribati', 'Grenada', 'Tonga', 'Seychelles', 'St. Vincent & Grenadines', 'Antigua and Barbuda',\n",
    "    'Andorra', 'Dominica', 'Saint Kitts & Nevis', 'Marshall Islands', 'Liechtenstein', 'Monaco', 'San Marino', 'Palau', 'Nauru',\n",
    "    'Tuvalu', 'Holy See'\n",
    "]\n",
    "\n",
    "# Extract unique countries/territories from the dataset\n",
    "unique_countries = data_org['Country'].unique()\n",
    "\n",
    "# Separate recognized countries and other territories\n",
    "recognized = [country for country in unique_countries if country in recognized_countries]\n",
    "territories = [country for country in unique_countries if country not in recognized_countries]\n",
    "\n",
    "# Print the lists\n",
    "print(\"Recognized Countries:\")\n",
    "print(sorted(recognized))\n",
    "print(len(recognized_countries))\n",
    "print(len(recognized))\n",
    "print(\"\\nTerritories:\")\n",
    "print(sorted(territories))\n",
    "print(len(territories))\n",
    "print(len(unique_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your Excel file\n",
    "file_path = r'C:\\Users\\91630\\OneDrive\\Desktop\\mcs4\\ens sem project\\World_development_measurement.xlsx'\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Calculate mean and variance for each column\n",
    "statistics = {'Column': [], 'Mean': [], 'Variance': []}\n",
    "\n",
    "for column in df.columns:\n",
    "    if df[column].dtype in [int, float]:  # Consider numeric columns only\n",
    "        mean_val = df[column].mean()\n",
    "        var_val = df[column].var()\n",
    "        statistics['Column'].append(column)\n",
    "        statistics['Mean'].append(mean_val)\n",
    "        statistics['Variance'].append(var_val)\n",
    "\n",
    "# Create a new DataFrame to display the results\n",
    "result_df = pd.DataFrame(statistics)\n",
    "\n",
    "# Print or display the result DataFrame\n",
    "print(\"Mean and Variance of each column:\")\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating copy of original dataset\n",
    "data = data_org.copy()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe90f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Country'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "# Perform one-sample t-test for health expenditure % GDP\n",
    "health_exp_values = df['Health Exp % GDP'].dropna()\n",
    "pop_mean = 0.06  # Hypothetical population mean (6%)\n",
    "t_stat, p_value = ttest_1samp(health_exp_values, pop_mean)\n",
    "print(t_stat)\n",
    "print(p_value)\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): Mean health expenditure % GDP is greater than 6%.\")\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H0): Mean health expenditure % GDP is less than or equal to 6%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Generate sample data for urban and rural populations (replace with your actual data)\n",
    "urban_population = df['Population Urban'].dropna()\n",
    "rural_population = 1 - urban_population  # Assuming complementary rural population\n",
    "\n",
    "# Perform two-sample t-test for urban vs. rural population proportions\n",
    "t_stat, p_value = ttest_ind(urban_population, rural_population)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): There is a significant difference between urban and rural population proportions.\")\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H0): There is no significant difference between urban and rural population proportions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba30195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Example: Split mobile phone usage data into two groups (replace with actual groups)\n",
    "group1 = df[df['Country'] == 'India']['Mobile Phone Usage'].dropna()\n",
    "group2 = df[df['Country'] == 'United States of America']['Mobile Phone Usage'].dropna()\n",
    "\n",
    "# Perform two-sample variance test (F-test)\n",
    "var_group1 = group1.var()\n",
    "var_group2 = group2.var()\n",
    "nobs_group1 = len(group1)\n",
    "nobs_group2 = len(group2)\n",
    "f_stat = var_group1 / var_group2\n",
    "p_value = 2 * (1 - stats.f.cdf(f_stat, dfn=nobs_group1 - 1, dfd=nobs_group2 - 1))\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): The variance of mobile phone usage in {} is significantly different from {}.\"\n",
    "          .format('India', 'United States of America'))\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H0): The variance of mobile phone usage in {} is similar to {}.\"\n",
    "          .format('India', 'United States of America'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Generate sample data for two groups (e.g., countries, regions)\n",
    "group1 = df[df['Country'] == 'Europe']['Infant Mortality Rate'].dropna()\n",
    "group2 = df[df['Country'] == 'Asia']['Infant Mortality Rate'].dropna()\n",
    "\n",
    "# Perform two-sample proportion test\n",
    "count = np.array([len(group1), len(group2)])\n",
    "nobs = np.array([len(df[df['Country'] == 'Europe']), len(df[df['Country'] == 'Asia'])])\n",
    "stat, p_value = proportions_ztest(count, nobs)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): There is a significant difference in infant mortality rates between Europe and Asia.\")\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H0): There is no significant difference in infant mortality rates between Europe and Asia.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Generate sample data for mobile phone usage\n",
    "mobile_usage = df['Mobile Phone Usage'].dropna()\n",
    "\n",
    "# Set the hypothetical proportion threshold (replace with your desired value)\n",
    "pop_prop = 0.5  # Hypothetical population proportion (50%)\n",
    "count = len(mobile_usage[mobile_usage > pop_prop])\n",
    "nobs = len(mobile_usage)\n",
    "\n",
    "# Perform one-sample proportion test\n",
    "stat, p_value = proportions_ztest(count, nobs, pop_prop)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): Proportion of mobile phone usage is greater than 50%.\")\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H0): Proportion of mobile phone usage is less than or equal to 50%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "\n",
    "\n",
    "energy_usage = df['Energy Usage'].dropna()\n",
    "\n",
    "# Perform goodness of fit test (normality test)\n",
    "stat, p_value = normaltest(energy_usage)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): Energy usage does not follow a normal distribution.\")\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H0): Energy usage follows a normal distribution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\91630\\OneDrive\\Desktop\\mcs4\\ens sem project\\World_development_measurement.xlsx\"\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "# Convert 'GDP' column to numeric (remove currency symbols and commas)\n",
    "df['GDP'] = pd.to_numeric(df['GDP'].replace('[\\$,]', '', regex=True), errors='coerce')\n",
    "\n",
    "# Remove rows with missing GDP values\n",
    "df = df.dropna(subset=['GDP'])\n",
    "\n",
    "# Discretize 'GDP' values into two bins\n",
    "df['GDP_bins'] = pd.cut(df['GDP'], bins=2)\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = pd.crosstab(df['Ease of Business'], df['GDP_bins'])\n",
    "\n",
    "# Perform chi-square test for independence\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): Ease of doing business is associated with GDP.\")\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H0): Ease of doing business is independent of GDP.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\91630\\OneDrive\\Desktop\\mcs4\\ens sem project\\World_development_measurement.xlsx\"\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "# Convert 'Birth Rate' and 'Infant Mortality Rate' columns to numeric\n",
    "df['Birth Rate'] = pd.to_numeric(df['Birth Rate'], errors='coerce')\n",
    "df['Infant Mortality Rate'] = pd.to_numeric(df['Infant Mortality Rate'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in 'Birth Rate' and 'Infant Mortality Rate' columns\n",
    "df = df.dropna(subset=['Birth Rate', 'Infant Mortality Rate'])\n",
    "\n",
    "# Discretize 'Birth Rate' and 'Infant Mortality Rate' columns into two bins each\n",
    "df['Birth Rate_bins'] = pd.cut(df['Birth Rate'], bins=2)\n",
    "df['Infant Mortality Rate_bins'] = pd.cut(df['Infant Mortality Rate'], bins=2)\n",
    "\n",
    "# Create contingency table for 'Birth Rate' and 'Infant Mortality Rate'\n",
    "contingency_table = pd.crosstab(df['Birth Rate_bins'], df['Infant Mortality Rate_bins'])\n",
    "\n",
    "# Perform chi-square test for independence\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): Birth rate is associated with infant mortality rate.\")\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H0): Birth rate is independent of infant mortality rate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b573092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\91630\\OneDrive\\Desktop\\mcs4\\ens sem project\\World_development_measurement.xlsx\"\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "# Convert 'Birth Rate', 'Life Expectancy Male', and 'Life Expectancy Female' columns to numeric\n",
    "df['Birth Rate'] = pd.to_numeric(df['Birth Rate'], errors='coerce')\n",
    "df['Life Expectancy Male'] = pd.to_numeric(df['Life Expectancy Male'], errors='coerce')\n",
    "df['Life Expectancy Female'] = pd.to_numeric(df['Life Expectancy Female'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in 'Birth Rate', 'Life Expectancy Male', and 'Life Expectancy Female' columns\n",
    "df = df.dropna(subset=['Birth Rate', 'Life Expectancy Male', 'Life Expectancy Female'])\n",
    "\n",
    "# Discretize 'Birth Rate', 'Life Expectancy Male', and 'Life Expectancy Female' columns into two bins each\n",
    "df['Birth Rate_bins'] = pd.cut(df['Birth Rate'], bins=2)\n",
    "df['Life Expectancy_bins'] = pd.cut((df['Life Expectancy Male'] + df['Life Expectancy Female']) / 2, bins=2)\n",
    "\n",
    "# Create contingency table for 'Birth Rate' and combined 'Life Expectancy' (averaged between male and female)\n",
    "contingency_table = pd.crosstab(df['Birth Rate_bins'], df['Life Expectancy_bins'])\n",
    "\n",
    "# Perform chi-square test for independence\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): Birth rate is associated with combined life expectancy (male and female).\")\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H0): Birth rate is independent of combined life expectancy (male and female).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\91630\\OneDrive\\Desktop\\mcs4\\ens sem project\\World_development_measurement.xlsx\"\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "# Convert 'Population Urban' and 'CO2 Emissions' columns to numeric\n",
    "df['Population Urban'] = pd.to_numeric(df['Population Urban'], errors='coerce')\n",
    "df['CO2 Emissions'] = pd.to_numeric(df['CO2 Emissions'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in 'Population Urban' and 'CO2 Emissions' columns\n",
    "df = df.dropna(subset=['Population Urban', 'CO2 Emissions'])\n",
    "\n",
    "# Discretize 'Population Urban' and 'CO2 Emissions' columns into two bins each\n",
    "df['Population Urban_bins'] = pd.cut(df['Population Urban'], bins=2)\n",
    "df['CO2 Emissions_bins'] = pd.cut(df['CO2 Emissions'], bins=2)\n",
    "\n",
    "# Create contingency table for 'Population Urban' and 'CO2 Emissions'\n",
    "contingency_table = pd.crosstab(df['Population Urban_bins'], df['CO2 Emissions_bins'])\n",
    "\n",
    "# Perform chi-square test for independence\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Interpret results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject Null Hypothesis (H0): Population urban is associated with CO2 emissions.\")\n",
    "else:\n",
    "    print(\"Fail to reject Null Hypothesis (H1): Population urban is independent of CO2 emissions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb85a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Data dictionary\n",
    "ht_data = {\n",
    "    'Type of Hypothesis': [\n",
    "        'One-Sample t-test for Health Expenditure % GDP',\n",
    "        'Two-Sample t-test for Urban vs. Rural Population Proportions',\n",
    "        'Two-Sample Variance Test (F-test) for Mobile Phone Usage',\n",
    "        'Two-Sample Proportion Test for Infant Mortality Rates (Europe vs. Asia)',\n",
    "        'One-Sample Proportion Test for Mobile Phone Usage',\n",
    "        'Goodness of Fit Test (Normality Test) for Energy Usage',\n",
    "        'Chi-Square Test for Independence (GDP vs. Ease of Business)',\n",
    "        'Chi-Square Test for Independence (Birth Rate vs. Infant Mortality Rate)',\n",
    "        'Chi-Square Test for Independence (Birth Rate vs. Life Expectancy of Male and Female)',\n",
    "        'Chi-Square Test for Independence (Population Urban vs. CO2 Emissions)'\n",
    "    ],\n",
    "    'Initial Conditions': [\n",
    "        'Population mean (6%), health_exp_values (sample data)',\n",
    "        'Sample data for urban and rural populations',\n",
    "        'Sample data for India and USA mobile phone usage',\n",
    "        'Sample data for infant mortality rates in Europe and Asia',\n",
    "        'Sample data for mobile phone usage',\n",
    "        'Sample data for energy usage',\n",
    "        'Sample data for GDP and Ease of Business',\n",
    "        'Sample data for birth rate and infant mortality rate',\n",
    "        'Sample data for Birth Rate and Life Expectancy of Male and Female',\n",
    "        'Sample data for Population Urban and CO2 emissions'\n",
    "    ],\n",
    "    \n",
    "    'Result': [\n",
    "        'Reject Null Hypothesis (H0): Mean health expenditure % GDP is greater than 6%.',\n",
    "        'Reject Null Hypothesis (H0): There is a significant difference between urban and rural population proportions.',\n",
    "        'Fail to reject Null Hypothesis (H0): The variance of mobile phone usage in India is similar to the variance in the United States of America.',\n",
    "        'Reject Null Hypothesis (H0): There is a significant difference in infant mortality rates between Europe and Asia.',\n",
    "        'Fail to reject Null Hypothesis (H0): Proportion of mobile phone usage is less than or equal to 50%.',\n",
    "        'Reject Null Hypothesis (H0): Energy usage does not follow a normal distribution.',\n",
    "        'Fail to reject Null Hypothesis (H0): Ease of doing business is independent of GDP.',\n",
    "        'Reject Null Hypothesis (H0): Birth rate is associated with infant mortality rate.',\n",
    "        'Reject Null Hypothesis (H0): Birth rate is associated with combined life expectancy (male and female).',\n",
    "        'Fail to reject Null Hypothesis (H0): Population urban is independent of CO2 emissions.'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(ht_data)\n",
    "\n",
    "# Generate HTML table representation with styling\n",
    "html_table = df_results.to_html(index=False, classes='my-table', justify='center')\n",
    "\n",
    "# Add custom CSS styles to the HTML table\n",
    "html_with_style = f\"\"\"\n",
    "    <div style=\"background-color: white; padding: 20px;\">\n",
    "        <style>\n",
    "            .my-table {{\n",
    "                width: 100%;\n",
    "                border-collapse: collapse;\n",
    "                border: 1px solid black;\n",
    "                color: black;  /* Text color */\n",
    "            }}\n",
    "            .my-table th, .my-table td {{\n",
    "                border: 1px solid #ddd;\n",
    "                padding: 8px;\n",
    "                text-align: left;\n",
    "                font-weight: bold;  /* Bold text */\n",
    "            }}\n",
    "            .my-table th {{\n",
    "                background-color: #f2f2f2;\n",
    "            }}\n",
    "            .my-table tr:nth-child(even) {{\n",
    "                background-color: #f9f9f9;\n",
    "            }}\n",
    "            .my-table tr:hover {{\n",
    "                background-color: #f1f1f1;\n",
    "            }}\n",
    "            .my-table td.result {{\n",
    "                color: black;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "        </style>\n",
    "        {html_table}\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "# Display HTML table with customized styling\n",
    "display(HTML(html_with_style))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>But here 'Country' column has a 208 number of unique categories.\n",
    "\n",
    "## Here, we use label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'data' contains the DataFrame with 'Country' and 'Country_encoded' columns\n",
    "# Create a DataFrame with unique country names\n",
    "unique_countries_df = pd.DataFrame({'Country': data['Country'].unique()})\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the \"Country\" column in the DataFrame\n",
    "unique_countries_df['Country_encoded'] = le.fit_transform(unique_countries_df['Country'])\n",
    "\n",
    "# Merge the encoded country data back into the original DataFrame 'data'\n",
    "data = pd.merge(data, unique_countries_df, on='Country', how='left')\n",
    "\n",
    "# Now convert 'Country_encoded' to float\n",
    "data['Country_encoded'] = data['Country_encoded'].astype(float)\n",
    "\n",
    "# Print the original country name along with its encoded value\n",
    "for country, encoded_value in zip(unique_countries_df['Country'], unique_countries_df['Country_encoded']):\n",
    "    print(f\"Country: {country}, Encoded Value: {encoded_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Country_encoded']=data['Country_encoded'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Country'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'Brown'> Some columns has $ and % attached we will have to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace '$', ',', and '%' with an empty string in selected columns\n",
    "columns_to_clean = ['GDP', 'Health Exp/Capita', 'Tourism Inbound', 'Tourism Outbound']\n",
    "data[columns_to_clean] = data[columns_to_clean].replace({'\\$': '', ',': '', '%': ''}, regex=True)\n",
    "\n",
    "# Convert columns to numeric data types\n",
    "data[columns_to_clean] = data[columns_to_clean].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Remove the '%' symbol from the 'Business Tax Rate' column\n",
    "data['Business Tax Rate'] = data['Business Tax Rate'].str.replace('%', '')\n",
    "\n",
    "# Convert 'Business Tax Rate' column to numeric data type\n",
    "data['Business Tax Rate'] = pd.to_numeric(data['Business Tax Rate'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=blue> All data type has same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check distribution for Numerical columns\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=blue> \"Number of Records\" have 0 standard deviation which means same values is present in each record.\n",
    "- <font color=blue> There are outliers present in a few features like 'Days to Start Business', 'Hours to do Tax' as their max value is much higher compared to the 75th percentile value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop \"Number of Records\" column\n",
    "data = data.drop(['Number of Records'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns\n",
    "data = data.rename(columns={'Birth Rate': 'BirthRate', 'Business Tax Rate': 'BusinessTaxRate','CO2 Emissions':'CO2Emissions','Days to Start Business':'DaystoStartBusiness','Ease of Business':'EaseofBusiness','Energy Usage':'EnergyUsage',\n",
    "                            'Health Exp % GDP':'HealthExpGDP','Health Exp/Capita':'HealthExpCapita','Hours to do Tax':'HourstodoTax','Infant Mortality Rate':'InfantMortalityRate','Internet Usage':'InternetUsage','Lending Interest':'LendingInterest',\n",
    "                            'Life Expectancy Female':'LifeExpectancyFemale','Life Expectancy Male':'LifeExpectancyMale','Mobile Phone Usage':'MobilePhoneUsage','Number of Records':'NumberofRecords','Population 0-14':'Population0to14',\n",
    "                            'Population 15-64':'Population15to64','Population 65+':'Populationmorethan65','Population Total':'PopulationTotal','Population Urban':'PopulationUrban','Tourism Inbound':'TourismInbound','Tourism Outbound':'TourismOutbound'})\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check missing values\n",
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=blue> Total 12203 null values in whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check null values in dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the null values for each attribute\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(data.isnull(),cmap='magma',yticklabels=False,cbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that,**\n",
    "- <font color=blue> 'Ease of bussiness' contain large number of missing values\n",
    "- <font color=blue> 'Population Urban' contain less number of missing values\n",
    "- <font color=blue> There is no missing values in 'Country' and 'Population Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize missing value for each artibute\n",
    "import missingno as msno\n",
    "msno.bar(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=brown> There are 26 null values in population urban column which is less than 1% values so droping those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove rows having null values\n",
    "data.dropna(subset=['PopulationUrban'],inplace=True)\n",
    "\n",
    "## check null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='blue'> There is no null values in Population Urban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>Now, we have to remove missing values of remaining columns.\n",
    "\n",
    "**The goal is to find out which is a better measure of the central tendency of data and use that value for replacing missing values appropriately.**\n",
    "- First we have to check distribution of all columns present in data.\n",
    "- If distribution of data is normal we use mean for replace missing values.\n",
    "- When the data is skewed, it is good to consider using the median value for\n",
    "  replacing the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution plots for all columns\n",
    "fig, axes=plt.subplots(6,4,figsize=(14,12),sharex=False,sharey=False)\n",
    "sns.distplot(data.BirthRate,ax=axes[0,0])\n",
    "sns.distplot(data.BusinessTaxRate,ax=axes[0,1])\n",
    "sns.distplot(data.CO2Emissions,ax=axes[0,2])\n",
    "sns.distplot(data.DaystoStartBusiness,ax=axes[0,3])\n",
    "sns.distplot(data.EaseofBusiness,ax=axes[1,0])\n",
    "sns.distplot(data.EnergyUsage,ax=axes[1,1])\n",
    "sns.distplot(data.GDP,ax=axes[1,2])\n",
    "sns.distplot(data.HealthExpGDP,ax=axes[1,3])\n",
    "sns.distplot(data.HealthExpCapita,ax=axes[2,0])\n",
    "sns.distplot(data.HourstodoTax,ax=axes[2,1])\n",
    "sns.distplot(data.InfantMortalityRate,ax=axes[2,2])\n",
    "sns.distplot(data.InternetUsage,ax=axes[2,3])\n",
    "sns.distplot(data.LendingInterest,ax=axes[3,0])\n",
    "sns.distplot(data.LifeExpectancyFemale,ax=axes[3,1])\n",
    "sns.distplot(data.LifeExpectancyMale,ax=axes[3,2])\n",
    "sns.distplot(data.MobilePhoneUsage,ax=axes[3,3])\n",
    "sns.distplot(data.MobilePhoneUsage,ax=axes[4,0])\n",
    "sns.distplot(data.Population0to14,ax=axes[4,1])\n",
    "sns.distplot(data.Population15to64,ax=axes[4,2])\n",
    "sns.distplot(data.Populationmorethan65,ax=axes[4,3])\n",
    "sns.distplot(data.PopulationTotal,ax=axes[5,0])\n",
    "sns.distplot(data.PopulationUrban,ax=axes[5,1])\n",
    "sns.distplot(data.TourismInbound,ax=axes[5,2])\n",
    "sns.distplot(data.TourismOutbound,ax=axes[5,3])\n",
    "plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above visvalization we can see that,\n",
    "- <font color=blue>'BusinessTaxRate', 'EaseofBusiness', 'HealthExpGDP', 'HourstodoTax' and 'Population0to14' columns has normal distribution so we replace missing values by mean.\n",
    "- <font color=blue>And for remaining columns with skewed data we replace missing values by median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace missing values by mean\n",
    "\n",
    "data['BusinessTaxRate'] = data['BusinessTaxRate'].fillna(data['BusinessTaxRate'].mean())\n",
    "data['EaseofBusiness'] = data['EaseofBusiness'].fillna(data['EaseofBusiness'].mean())\n",
    "data['HealthExpGDP'] = data['HealthExpGDP'].fillna(data['HealthExpGDP'].mean())\n",
    "data['HourstodoTax'] = data['HourstodoTax'].fillna(data['HourstodoTax'].mean())\n",
    "data['Population0to14'] = data['Population0to14'].fillna(data['Population0to14'].mean())\n",
    "\n",
    "## Replace missing values by median\n",
    "\n",
    "data['BirthRate'] = data['BirthRate'].fillna(data['BirthRate'].median())\n",
    "data['CO2Emissions'] = data['CO2Emissions'].fillna(data['CO2Emissions'].median())\n",
    "data['DaystoStartBusiness'] = data['DaystoStartBusiness'].fillna(data['DaystoStartBusiness'].median())\n",
    "data['EnergyUsage'] = data['EnergyUsage'].fillna(data['EnergyUsage'].median())\n",
    "data['HealthExpCapita']=data['HealthExpCapita'].fillna(data['HealthExpCapita'].mean())\n",
    "data['GDP'] = data['GDP'].fillna(data['GDP'].median())\n",
    "data['InfantMortalityRate'] = data['InfantMortalityRate'].fillna(data['InfantMortalityRate'].median())\n",
    "data['InternetUsage'] = data['InternetUsage'].fillna(data['InternetUsage'].median())\n",
    "data['LendingInterest'] = data['LendingInterest'].fillna(data['LendingInterest'].median())\n",
    "data['LifeExpectancyFemale'] = data['LifeExpectancyFemale'].fillna(data['LifeExpectancyFemale'].median())\n",
    "data['LifeExpectancyMale'] = data['LifeExpectancyMale'].fillna(data['LifeExpectancyMale'].median())\n",
    "data['MobilePhoneUsage'] = data['MobilePhoneUsage'].fillna(data['MobilePhoneUsage'].median())\n",
    "data['TourismInbound'] = data['TourismInbound'].fillna(data['TourismInbound'].median())\n",
    "data['TourismOutbound'] = data['TourismOutbound'].fillna(data['TourismOutbound'].median())\n",
    "data['Population15to64'] = data['Population15to64'].fillna(data['Population15to64'].median())\n",
    "data['Populationmorethan65'] = data['Populationmorethan65'].fillna(data['Populationmorethan65'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Country_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check missising values\n",
    "print(\"{} missing values present in whole data.\".format(data.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualise relation between variables\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(data.corr(),annot=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Correlation Map of variables\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- <font color=blue> 'Population 0 to 14' and 'Birth Rate' has strong relation.\n",
    "- <font color=blue> 'Population 15 to 64' and 'Birth Rate' has weak relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making copy of data\n",
    "data1 = data.copy()    # For method 1\n",
    "data2 = data.copy()    # For method 2\n",
    "data3 = data.copy()    # For method 3\n",
    "data4=data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=Magenta>Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check outliers present in data1\n",
    "fig, axes=plt.subplots(6,4,figsize=(14,12),sharex=False,sharey=False)\n",
    "sns.boxplot(data1.BirthRate,ax=axes[0,0])\n",
    "sns.boxplot(data1.BusinessTaxRate,ax=axes[0,1])\n",
    "sns.boxplot(data1.CO2Emissions,ax=axes[0,2])\n",
    "sns.boxplot(data1.DaystoStartBusiness,ax=axes[0,3])\n",
    "sns.boxplot(data1.EaseofBusiness,ax=axes[1,0])\n",
    "sns.boxplot(data1.EnergyUsage,ax=axes[1,1])\n",
    "sns.boxplot(data1.GDP,ax=axes[1,2])\n",
    "sns.boxplot(data1.HealthExpGDP,ax=axes[1,3])\n",
    "sns.boxplot(data1.HealthExpCapita,ax=axes[2,0])\n",
    "sns.boxplot(data1.HourstodoTax,ax=axes[2,1])\n",
    "sns.boxplot(data1.InfantMortalityRate,ax=axes[2,2])\n",
    "sns.boxplot(data1.InternetUsage,ax=axes[2,3])\n",
    "sns.boxplot(data1.LendingInterest,ax=axes[3,0])\n",
    "sns.boxplot(data1.LifeExpectancyFemale,ax=axes[3,1])\n",
    "sns.boxplot(data1.LifeExpectancyMale,ax=axes[3,2])\n",
    "sns.boxplot(data1.MobilePhoneUsage,ax=axes[3,3])\n",
    "sns.boxplot(data1.MobilePhoneUsage,ax=axes[4,0])\n",
    "sns.boxplot(data1.Population0to14,ax=axes[4,1])\n",
    "sns.boxplot(data1.Population15to64,ax=axes[4,2])\n",
    "sns.boxplot(data1.Populationmorethan65,ax=axes[4,3])\n",
    "sns.boxplot(data1.PopulationTotal,ax=axes[5,0])\n",
    "sns.boxplot(data1.PopulationUrban,ax=axes[5,1])\n",
    "sns.boxplot(data1.TourismInbound,ax=axes[5,2])\n",
    "sns.boxplot(data1.TourismOutbound,ax=axes[5,3])\n",
    "plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adbe7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming 'data4' is your original DataFrame\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Transform the data (scaling each column to -1 to 1)\n",
    "data4_scaled = scaler.fit_transform(data4)\n",
    "\n",
    "# Convert NumPy array back to DataFrame\n",
    "data4_scaled_df = pd.DataFrame(data4_scaled, columns=data4.columns)\n",
    "\n",
    "# Loop through each column in the scaled DataFrame\n",
    "for col in data4_scaled_df.columns:\n",
    "    # Create the density plot using the scaled data\n",
    "    sns.kdeplot(data4_scaled_df[col])\n",
    "    \n",
    "    # Optional: Customize the plot\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Density Plot for Column: {col}')\n",
    "    plt.show()\n",
    "    plt.clf()  # Clear the current figure to prepare for the next plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- <font color=blue> Some columns like \"Population Total\", \"Tourism in bound\", \"Tourism out bound\" has large number of outlier present.\n",
    "- <font color=blue> columns like \"Population Urban\", \"Population 0 to 14\" has less number of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "here, we are removing outliers using inter quratile range method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outlier removal\n",
    "Q1 = data1.quantile(0.25)\n",
    "Q3 = data1.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "data1 = data1[~((data1 < (Q1 - 1.5 * IQR)) | (data1 > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By outlier removal we lost 60% of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magenta'> Scaling</font>\n",
    "   &nbsp;   Scaling is a technique to standardize the independent features present in the data in a fixed range. We do this to make sure all the features are in same scale.</font>\n",
    " \n",
    "There are two types of scaling\n",
    " - Standard Scaling \n",
    " - MinMax Scaling\n",
    "\n",
    "#### Here we will be using  Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardization of data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scale_data = scaler.fit_transform(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the list of attributes for clustering\n",
    "attributes = [\n",
    "    'BirthRate', 'BusinessTaxRate', 'CO2Emissions', 'DaystoStartBusiness',\n",
    "    'EaseofBusiness', 'EnergyUsage', 'GDP', 'HealthExpGDP', 'HealthExpCapita',\n",
    "    'HourstodoTax', 'InfantMortalityRate', 'InternetUsage', 'LendingInterest',\n",
    "    'LifeExpectancyFemale', 'LifeExpectancyMale', 'MobilePhoneUsage',\n",
    "    'Population0to14', 'Population15to64', 'Populationmorethan65',\n",
    "    'PopulationTotal', 'PopulationUrban', 'TourismInbound', 'TourismOutbound'\n",
    "]\n",
    "\n",
    "# Select and preprocess the data\n",
    "data_cleaned = data[attributes].copy()\n",
    "\n",
    "# Impute missing values using mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_cleaned[attributes] = imputer.fit_transform(data_cleaned[attributes])\n",
    "\n",
    "# Scale the data to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_cleaned[attributes])\n",
    "\n",
    "# Perform KMeans clustering with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "data_cleaned['Cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Calculate the mean of each attribute by cluster\n",
    "cluster_means = data_cleaned.groupby('Cluster')[attributes].mean()\n",
    "\n",
    "# Count the number of data points in each cluster\n",
    "cluster_counts = data_cleaned['Cluster'].value_counts().sort_index()\n",
    "\n",
    "# Prepare the data for the summary DataFrame\n",
    "summary_data = []\n",
    "\n",
    "# Populate the summary data list with attribute means and counts by cluster\n",
    "for attr in attributes:\n",
    "    row = {\n",
    "        'Attribute': attr,\n",
    "        'Overall Mean': data_cleaned[attr].mean(),\n",
    "        'Cluster 0 Mean': cluster_means.loc[0, attr],\n",
    "        'Cluster 1 Mean': cluster_means.loc[1, attr],\n",
    "        'Cluster 2 Mean': cluster_means.loc[2, attr]\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "# Create the DataFrame from the list of dictionaries\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Add the count information to the column headers for Cluster 1 Mean and Cluster 2 Mean\n",
    "summary_df.columns = [\n",
    "    'Attribute', 'Overall Mean', f'Cluster 0 Mean({cluster_counts[0]}counts)',\n",
    "    f'Cluster 1 Mean ({cluster_counts[1]} counts)', \n",
    "    f'Cluster 2 Mean ({cluster_counts[2]} counts)'\n",
    "]\n",
    "\n",
    "summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pc = PCA()\n",
    "pc_components = pc.fit_transform(scale_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of variance that each PCA explains is\n",
    "pc.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in percentage - The amount of variance that each PCA explains is\n",
    "var = pc.explained_variance_ratio_\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative variance\n",
    "var1 = np.cumsum(np.round(var,decimals=4)*100)\n",
    "var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance plot for PCA components obtained\n",
    "plt.plot(var1,color='red')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Cumulative Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at graph we can decide how much percentage we want and accordingly go for that much column numbers.\n",
    "here, we are taking 15 columns because they are giving more than 95% data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = pc_components[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming pc_components is the PCA-transformed data with 15 components\n",
    "# Perform PCA transformation\n",
    "pc = PCA(n_components=15)  # Specify the number of components you want\n",
    "pc_components = pc.fit_transform(scale_data)\n",
    "\n",
    "# Extract the first three principal components\n",
    "pc1 = pc_components[:, 0]\n",
    "pc2 = pc_components[:, 1]\n",
    "pc3 = pc_components[:, 2]\n",
    "\n",
    "# Get the PCA components (loadings)\n",
    "pca_components = pc.components_[:3].T  # Transpose to align with feature names\n",
    "\n",
    "# Display feature contributions in the first three principal components\n",
    "features = ['BirthRate', 'BusinessTaxRate', 'CO2Emissions', 'DaystoStartBusiness',\n",
    "            'EaseofBusiness', 'EnergyUsage', 'GDP', 'HealthExpGDP', 'HealthExpCapita',\n",
    "            'HourstodoTax', 'InfantMortalityRate', 'InternetUsage', 'LendingInterest',\n",
    "            'LifeExpectancyFemale', 'LifeExpectancyMale']  # Update with actual feature names\n",
    "\n",
    "plt.figure(figsize=(15, 6))  # Adjust figure size as needed\n",
    "for i, (pc, component) in enumerate(zip([pc1, pc2, pc3], pca_components.T), 1):\n",
    "    # Trim component array to match the number of features\n",
    "    component_trimmed = component[:len(features)]\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.barh(features, component_trimmed, color='skyblue')\n",
    "    plt.xlabel('Contribution')\n",
    "    plt.title(f'Principal Component {i} - Explained Variance: {pc.var()*100:.2f}%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot between PCA's\n",
    "x=pc_components[:,0]\n",
    "y=pc_components[:,1]\n",
    "z=pc_components[:,2]\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.scatter(x,z)\n",
    "plt.scatter(y,z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='magenta'> Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are various types of clustring mostly used are :**\n",
    "   - k-means clustring\n",
    "   - Hierarchy clustring\n",
    "   - DBSCAN clustring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'> K-means Clustring\n",
    "#### Using the elbow method to find the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(data_pca)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, \n",
    "- 3 is the elbow point.\n",
    "- We have to create three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating clusters\n",
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_pca[y_kmeans == 0, 0], data_pca[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(data_pca[y_kmeans == 1, 0], data_pca[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(data_pca[y_kmeans == 2, 0], data_pca[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.title('Clusters of measurements')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d540433",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_and_encoded_values = {\n",
    "    'Algeria': 2, 'Angola': 5, 'Benin': 20, 'Botswana': 25, 'Burkina Faso': 29,\n",
    "    'Burundi': 30, 'Cameroon': 32, 'Central African Republic': 35, 'Chad': 36,\n",
    "    'Comoros': 40, 'Congo, Dem. Rep.': 41, 'Congo, Rep.': 42, 'Cote d\\'Ivoire': 44,\n",
    "    'Djibouti': 51, 'Egypt, Arab Rep.': 55, 'Equatorial Guinea': 57, 'Eritrea': 58,\n",
    "    'Ethiopia': 60, 'Gabon': 66, 'Gambia, The': 67, 'Ghana': 70, 'Guinea': 76,\n",
    "    'Guinea-Bissau': 77, 'Kenya': 96, 'Lesotho': 106, 'Liberia': 107, 'Libya': 108,\n",
    "    'Madagascar': 114, 'Malawi': 115, 'Mali': 118, 'Mauritania': 121, 'Mauritius': 122,\n",
    "    'Morocco': 129, 'Mozambique': 130, 'Namibia': 132, 'Niger': 138, 'Nigeria': 139,\n",
    "    'Rwanda': 154, 'Sao Tome and Principe': 157, 'Senegal': 159, 'Seychelles': 161,\n",
    "    'Sierra Leone': 162, 'Somalia': 168, 'South Africa': 169, 'South Sudan': 170,\n",
    "    'Sudan': 177, 'Swaziland': 179, 'Tanzania': 184, 'Togo': 187, 'Tunisia': 190,\n",
    "    'Uganda': 194, 'Zambia': 206, 'Zimbabwe': 207, 'Afghanistan': 0, 'Armenia': 8,\n",
    "    'Azerbaijan': 12, 'Bangladesh': 15, 'Bhutan': 22, 'Brunei Darussalam': 27,\n",
    "    'Cambodia': 31, 'China': 38, 'Georgia': 68, 'Hong Kong SAR, China': 81,\n",
    "    'India': 84, 'Indonesia': 85, 'Japan': 93, 'Kazakhstan': 95, 'Korea, Dem. Rep.': 98,\n",
    "    'Korea, Rep.': 99, 'Kyrgyz Republic': 102, 'Lao PDR': 103, 'Macao SAR, China': 112,\n",
    "    'Malaysia': 116, 'Maldives': 117, 'Mongolia': 127, 'Myanmar': 131, 'Nepal': 133,\n",
    "    'Pakistan': 142, 'Philippines': 147, 'Singapore': 163, 'Sri Lanka': 172,\n",
    "    'Tajikistan': 183, 'Thailand': 185, 'Timor-Leste': 186, 'Turkmenistan': 192,\n",
    "    'Uzbekistan': 200, 'Vietnam': 203, 'Albania': 1, 'Andorra': 4, 'Austria': 11,\n",
    "    'Belarus': 17, 'Belgium': 18, 'Bosnia and Herzegovina': 24, 'Bulgaria': 28,\n",
    "    'Croatia': 45, 'Cyprus': 48, 'Czech Republic': 49, 'Denmark': 50, 'Estonia': 59,\n",
    "    'Faeroe Islands': 61, 'Finland': 63, 'France': 64, 'Germany': 69, 'Greece': 71,\n",
    "    'Hungary': 82, 'Iceland': 83, 'Ireland': 88, 'Isle of Man': 89, 'Italy': 91,\n",
    "    'Kosovo': 100, 'Latvia': 104, 'Liechtenstein': 109, 'Lithuania': 110,\n",
    "    'Luxembourg': 111, 'Macedonia, FYR': 113, 'Malta': 119, 'Moldova': 125,\n",
    "    'Monaco': 126, 'Montenegro': 128, 'Netherlands': 134, 'Norway': 140, 'Poland': 148,\n",
    "    'Portugal': 149, 'Romania': 152, 'Russian Federation': 153, 'San Marino': 156,\n",
    "    'Serbia': 160, 'Slovak Republic': 165, 'Slovenia': 166, 'Spain': 171, 'Sweden': 180,\n",
    "    'Switzerland': 181, 'Turkey': 191, 'Ukraine': 195, 'United Kingdom': 197,\n",
    "    'Bahrain': 14, 'Iran, Islamic Rep.': 86, 'Iraq': 87, 'Israel': 90, 'Jordan': 94,\n",
    "    'Kuwait': 101, 'Lebanon': 105, 'Oman': 141, 'Qatar': 151, 'Saudi Arabia': 158,\n",
    "    'Syrian Arab Republic': 182, 'United Arab Emirates': 196, 'Yemen, Rep.': 205,\n",
    "    'American Samoa': 3, 'Australia': 10, 'Fiji': 62, 'French Polynesia': 65,\n",
    "    'Guam': 74, 'Kiribati': 97, 'Marshall Islands': 120, 'Micronesia, Fed. Sts.': 124,\n",
    "    'New Caledonia': 135, 'New Zealand': 136, 'Papua New Guinea': 144, 'Samoa': 155,\n",
    "    'Solomon Islands': 167, 'Tonga': 188, 'Vanuatu': 201, 'Antigua and Barbuda': 6,\n",
    "    'Argentina': 7, 'Aruba': 9, 'Bahamas, The': 13, 'Barbados': 16, 'Belize': 19,\n",
    "    'Bermuda': 21, 'Bolivia': 23, 'Brazil': 26, 'Canada': 33, 'Cayman Islands': 34,\n",
    "    'Chile': 37, 'Colombia': 39, 'Costa Rica': 43, 'Cuba': 46, 'Curacao': 47,\n",
    "    'Dominica': 52, 'Dominican Republic': 53, 'Ecuador': 54, 'El Salvador': 56,\n",
    "    'Greenland': 72, 'Grenada': 73, 'Guatemala': 75, 'Guyana': 78, 'Haiti': 79,\n",
    "    'Honduras': 80, 'Jamaica': 92, 'Mexico': 123, 'Nicaragua': 137, 'Panama': 143,\n",
    "    'Paraguay': 145, 'Peru': 146, 'Puerto Rico': 150, 'Sint Maarten (Dutch part)': 164,\n",
    "    'St. Kitts and Nevis': 173, 'St. Lucia': 174, 'St. Martin (French part)': 175,\n",
    "    'St. Vincent and the Grenadines': 176, 'Suriname': 178, 'Trinidad and Tobago': 189,\n",
    "    'Turks and Caicos Islands': 193, 'United States': 198, 'Uruguay': 199,\n",
    "    'Venezuela, RB': 202, 'Virgin Islands (U.S.)': 204\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d29587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming 'data_pca' is your PCA-transformed data and 'country_and_encoded_values' is your dictionary\n",
    "# Extract country names and their encoded values\n",
    "country_names = list(country_and_encoded_values.keys())\n",
    "encoded_values = list(country_and_encoded_values.values())\n",
    "\n",
    "# Perform k-means clustering on 'data_pca'\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(data_pca)\n",
    "\n",
    "# Create a DataFrame mapping encoded values to country names\n",
    "country_cluster_df = pd.DataFrame({'Country': country_names, 'Encoded_Value': encoded_values})\n",
    "\n",
    "# Merge the country_cluster_df with the cluster labels\n",
    "country_cluster_df['Cluster'] = pd.Series(y_kmeans)\n",
    "\n",
    "# Display the DataFrame with country names, encoded values, and cluster labels\n",
    "print(country_cluster_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the three clusters of measurments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy check\n",
    "from sklearn.metrics import silhouette_score\n",
    "s1_kmeans = silhouette_score(data_pca, y_kmeans)\n",
    "print('Silhouette Score for K-means clustring :', s1_kmeans)\n",
    "inertia_value = kmeans.inertia_\n",
    "print('Inertia for K-means clustering:', inertia_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221064c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "k_values = [ 3]\n",
    "accuracy_results = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_pca)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Check cluster_centers array shape and contents\n",
    "    print(f\"Cluster centers for k={k}:\")\n",
    "    print(cluster_centers)\n",
    "    print(f\"Shape: {cluster_centers.shape}\")\n",
    "    \n",
    "    # Calculate pairwise distances from each point to its cluster centroid\n",
    "    distances = pairwise_distances(data_pca, cluster_centers, metric='euclidean')\n",
    "    \n",
    "    cluster_accuracy = []\n",
    "    \n",
    "    # Calculate accuracy for each cluster\n",
    "    for cluster_idx in range(k):\n",
    "        # Get distances for current cluster\n",
    "        cluster_distances = distances[:, cluster_idx]\n",
    "        \n",
    "        # Sort distances and get indices\n",
    "        sorted_indices = np.argsort(cluster_distances)\n",
    "        \n",
    "        # Number of points in the current cluster\n",
    "        num_points_in_cluster = np.sum(cluster_labels == cluster_idx)\n",
    "        \n",
    "        # Select top num_points_in_cluster distances\n",
    "        nearest_distances_indices = sorted_indices[:num_points_in_cluster]\n",
    "        \n",
    "        # Count correct labels (same as cluster label)\n",
    "        correct_labels_count = np.sum(cluster_labels[nearest_distances_indices] == cluster_idx)\n",
    "        \n",
    "        # Calculate accuracy for the current cluster\n",
    "        cluster_accuracy.append(correct_labels_count / num_points_in_cluster)\n",
    "    \n",
    "    # Calculate overall accuracy (mean of cluster accuracies)\n",
    "    overall_accuracy_M1_kmeans = np.mean(cluster_accuracy)\n",
    "    accuracy_results.append(overall_accuracy_M1_kmeans)\n",
    "    print(\"Accuracy Results:\",overall_accuracy_M1_kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a511e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "spectral = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)\n",
    "y_spectral = spectral.fit_predict(data_pca)\n",
    "\n",
    "plt.scatter(data_pca[y_spectral == 0, 0], data_pca[y_spectral == 0, 1], s=100, c='red', label='Cluster 1')\n",
    "plt.scatter(data_pca[y_spectral == 1, 0], data_pca[y_spectral == 1, 1], s=100, c='blue', label='Cluster 2')\n",
    "plt.scatter(data_pca[y_spectral == 2, 0], data_pca[y_spectral == 2, 1], s=100, c='green', label='Cluster 3')\n",
    "plt.title('Clusters of measurements')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "s1_spectral = silhouette_score(data_pca, y_spectral)\n",
    "print('Silhouette Score for Spectral clustering:', s1_spectral)\n",
    "\n",
    "# Checking accuracy for Spectral Clustering\n",
    "k = 3\n",
    "spectral = SpectralClustering(n_clusters=k, affinity='nearest_neighbors', random_state=42)\n",
    "cluster_labels_spectral = spectral.fit_predict(data_pca)\n",
    "\n",
    "distances_spectral = pairwise_distances(data_pca, cluster_centers, metric='euclidean')\n",
    "cluster_accuracy_spectral = []\n",
    "\n",
    "for cluster_idx in range(k):\n",
    "    cluster_distances_spectral = distances_spectral[:, cluster_idx]\n",
    "    sorted_indices_spectral = np.argsort(cluster_distances_spectral)\n",
    "    num_points_in_cluster_spectral = np.sum(cluster_labels_spectral == cluster_idx)\n",
    "    nearest_distances_indices_spectral = sorted_indices_spectral[:num_points_in_cluster_spectral]\n",
    "    correct_labels_count_spectral = np.sum(cluster_labels_spectral[nearest_distances_indices_spectral] == cluster_idx)\n",
    "    cluster_accuracy_spectral.append(correct_labels_count_spectral / num_points_in_cluster_spectral)\n",
    "\n",
    "overall_accuracy_M1_spectral = np.mean(cluster_accuracy_spectral)\n",
    "print(\"Accuracy Results for Spectral Clustering:\", overall_accuracy_M1_spectral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'> Hierarchy Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "dendrogram = sch.dendrogram(sch.linkage(data_pca, method = 'ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import numpy as np\n",
    "\n",
    "# Compute the hierarchical clustering\n",
    "Z = linkage(data_pca, method='ward')\n",
    "\n",
    "# Determine cluster labels\n",
    "n_clusters = 4\n",
    "y_hc = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "\n",
    "print(\"Cluster labels:\", y_hc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check unique cluster labels\n",
    "unique_labels = np.unique(y_hc)\n",
    "print(\"Unique cluster labels:\", unique_labels)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in unique_labels:\n",
    "    plt.scatter(data_pca[y_hc == label, 0], data_pca[y_hc == label, 1], label=f'Cluster {label}')\n",
    "\n",
    "plt.title('Clusters of measurements')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f66086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame mapping encoded values to country names\n",
    "country_cluster_df = pd.DataFrame({'Country': country_names, 'Encoded_Value': encoded_values})\n",
    "\n",
    "# Merge the country_cluster_df with the cluster labels\n",
    "country_cluster_df['Cluster'] = pd.Series(y_hc)\n",
    "\n",
    "# Display the DataFrame with country names, encoded values, and cluster labels\n",
    "print(country_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy check\n",
    "s1_hierarchy = silhouette_score(data_pca,y_hc)\n",
    "print('Silhouette Score for Hierarchy clustring :',s1_hierarchy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c955827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Perform Hierarchical Clustering\n",
    "n_clusters = 4\n",
    "hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hc_labels = hc.fit_predict(data_pca)\n",
    "\n",
    "# Calculate cluster centers\n",
    "cluster_centers = np.array([data_pca[hc_labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "\n",
    "# Check cluster_centers array shape and contents\n",
    "print(f\"Cluster centers for n_clusters={n_clusters}:\")\n",
    "print(cluster_centers)\n",
    "print(f\"Shape: {cluster_centers.shape}\")\n",
    "\n",
    "# Calculate pairwise distances from each point to its cluster centroid\n",
    "distances = pairwise_distances(data_pca, cluster_centers, metric='euclidean')\n",
    "\n",
    "cluster_accuracy = []\n",
    "\n",
    "# Calculate accuracy for each cluster\n",
    "for cluster_idx in range(n_clusters):\n",
    "    # Get distances for current cluster\n",
    "    cluster_distances = distances[:, cluster_idx]\n",
    "    \n",
    "    # Sort distances and get indices\n",
    "    sorted_indices = np.argsort(cluster_distances)\n",
    "    \n",
    "    # Number of points in the current cluster\n",
    "    num_points_in_cluster = np.sum(hc_labels == cluster_idx)\n",
    "    \n",
    "    # Select top num_points_in_cluster distances\n",
    "    nearest_distances_indices = sorted_indices[:num_points_in_cluster]\n",
    "    \n",
    "    # Count correct labels (same as cluster label)\n",
    "    correct_labels_count = np.sum(hc_labels[nearest_distances_indices] == cluster_idx)\n",
    "    \n",
    "    # Calculate accuracy for the current cluster\n",
    "    cluster_accuracy.append(correct_labels_count / num_points_in_cluster)\n",
    "\n",
    "# Calculate overall accuracy (mean of cluster accuracies)\n",
    "overall_accuracy_M1_hierarchial = np.mean(cluster_accuracy)\n",
    "print(f\"Accuracy for Hierarchical Clustering with n_clusters={n_clusters}: {overall_accuracy_M1_hierarchial:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'> DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "eps = 0.2\n",
    "min_samples = 5\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "# Fit the data to obtain clustering labels\n",
    "dbscan_labels = dbscan.fit_predict(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf33883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=dbscan_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame mapping encoded values to country names\n",
    "country_cluster_df = pd.DataFrame({'Country': country_names, 'Encoded_Value': encoded_values})\n",
    "\n",
    "# Merge the country_cluster_df with the cluster labels\n",
    "country_cluster_df['Cluster'] = pd.Series(dbscan_labels)\n",
    "\n",
    "# Display the DataFrame with country names, encoded values, and cluster labels\n",
    "print(country_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['cluster']=dbscan.labels_\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d2bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "eps = 0.2\n",
    "min_samples = 5\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan_labels = dbscan.fit_predict(data_pca)\n",
    "\n",
    "# Extract unique cluster labels (excluding noise)\n",
    "\n",
    "# Calculate cluster centers\n",
    "cluster_centers = np.array([data_pca[dbscan_labels == cluster].mean(axis=0) for cluster in dbscan_labels])\n",
    "\n",
    "# Check cluster_centers array shape and contents\n",
    "print(f\"Cluster centers for DBSCAN (excluding noise):\")\n",
    "print(cluster_centers)\n",
    "print(f\"Shape: {cluster_centers.shape}\")\n",
    "\n",
    "# Calculate pairwise distances from each point to its cluster centroid\n",
    "distances = pairwise_distances(data_pca, cluster_centers, metric='euclidean')\n",
    "\n",
    "cluster_accuracy = []\n",
    "\n",
    "# Calculate accuracy for each cluster\n",
    "for cluster_idx, cluster_label in enumerate(dbscan_labels):\n",
    "    # Get distances for current cluster\n",
    "    cluster_distances = distances[:, cluster_idx]\n",
    "    \n",
    "    # Sort distances and get indices\n",
    "    sorted_indices = np.argsort(cluster_distances)\n",
    "    \n",
    "    # Number of points in the current cluster\n",
    "    num_points_in_cluster = np.sum(dbscan_labels == cluster_label)\n",
    "    \n",
    "    # Select top num_points_in_cluster distances\n",
    "    nearest_distances_indices = sorted_indices[:num_points_in_cluster]\n",
    "    \n",
    "    # Count correct labels (same as cluster label)\n",
    "    correct_labels_count = np.sum(dbscan_labels[nearest_distances_indices] == cluster_label)\n",
    "    \n",
    "    # Calculate accuracy for the current cluster\n",
    "    cluster_accuracy.append(correct_labels_count / num_points_in_cluster)\n",
    "\n",
    "# Calculate overall accuracy (mean of cluster accuracies)\n",
    "overall_accuracy_M1_DBSCAN = np.mean(cluster_accuracy)\n",
    "print(f\"Accuracy for DBSCAN with eps={eps}, min_samples={min_samples}: {overall_accuracy_M1_DBSCAN:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas filtering and get noisy datapoints -1\n",
    "data1[data1['cluster']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_dbscan = silhouette_score(data_pca, dbscan_labels)\n",
    "print(\"Silhouette Score for DBSCAN is:\", s1_dbscan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>DBSCAN is not most effective on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get clustrers but, we used outlier removal data. Problem with that data is we have only 40% data left after removing outliers. Which means we loss more than 50% information from out dataset. So, we have to keep outliers. Now, create clusters with this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Method 2\n",
    "**With ouliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardization of data\n",
    "scaler = StandardScaler()\n",
    "scale_data = scaler.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PCA()\n",
    "pc_components = pc.fit_transform(scale_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in percentage - The amount of variance that each PCA explains is\n",
    "var = pc.explained_variance_ratio_\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative variance\n",
    "var1 = np.cumsum(np.round(var,decimals=4)*100)\n",
    "var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance plot for PCA components obtained\n",
    "plt.plot(var1,color='red')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Cumulative Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at graph we can decide how much percentage we want and accordingly go for that much column numbers.\n",
    "here, we are taking 15 columns because they are giving more than 95% data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca2 = pc_components[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming pc_components is the PCA-transformed data with 15 components\n",
    "# Perform PCA transformation\n",
    "pc = PCA(n_components=15)  # Specify the number of components you want\n",
    "pc_components = pc.fit_transform(scale_data)\n",
    "\n",
    "# Extract the first three principal components\n",
    "pc1 = pc_components[:, 0]\n",
    "pc2 = pc_components[:, 1]\n",
    "pc3 = pc_components[:, 2]\n",
    "\n",
    "# Get the PCA components (loadings)\n",
    "pca_components = pc.components_[:3].T  # Transpose to align with feature names\n",
    "\n",
    "# Display feature contributions in the first three principal components\n",
    "features = ['BirthRate', 'BusinessTaxRate', 'CO2Emissions', 'DaystoStartBusiness',\n",
    "            'EaseofBusiness', 'EnergyUsage', 'GDP', 'HealthExpGDP', 'HealthExpCapita',\n",
    "            'HourstodoTax', 'InfantMortalityRate', 'InternetUsage', 'LendingInterest',\n",
    "            'LifeExpectancyFemale', 'LifeExpectancyMale']  # Update with actual feature names\n",
    "\n",
    "plt.figure(figsize=(15, 6))  # Adjust figure size as needed\n",
    "for i, (pc, component) in enumerate(zip([pc1, pc2, pc3], pca_components.T), 1):\n",
    "    # Trim component array to match the number of features\n",
    "    component_trimmed = component[:len(features)]\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.barh(features, component_trimmed, color='skyblue')\n",
    "    plt.xlabel('Contribution')\n",
    "    plt.title(f'Principal Component {i} - Explained Variance: {pc.var()*100:.2f}%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'> K-means Clustring\n",
    "#### Using the elbow method to find the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(data_pca2)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, \n",
    "- 3 is the elbow point.\n",
    "- We have to create three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating clusters\n",
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(data_pca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_pca2[y_kmeans == 0, 0], data_pca2[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(data_pca2[y_kmeans == 1, 0], data_pca2[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(data_pca2[y_kmeans == 2, 0], data_pca2[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.title('Clusters of measurements')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a DataFrame mapping encoded values to country names\n",
    "country_cluster_df = pd.DataFrame({'Country': country_names, 'Encoded_Value': encoded_values})\n",
    "\n",
    "# Merge the country_cluster_df with the cluster labels\n",
    "country_cluster_df['Cluster'] = pd.Series(y_kmeans)\n",
    "\n",
    "# Display the DataFrame with country names, encoded values, and cluster labels\n",
    "print(country_cluster_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy Check\n",
    "s2_kmeans = silhouette_score(data_pca2, y_kmeans)\n",
    "print('Silhouette Score for K-means clustring :', s2_kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4824a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "k_values = [ 3]\n",
    "accuracy_results = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_pca2)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Check cluster_centers array shape and contents\n",
    "    print(f\"Cluster centers for k={k}:\")\n",
    "    print(cluster_centers)\n",
    "    print(f\"Shape: {cluster_centers.shape}\")\n",
    "    \n",
    "    # Calculate pairwise distances from each point to its cluster centroid\n",
    "    distances = pairwise_distances(data_pca2, cluster_centers, metric='euclidean')\n",
    "    \n",
    "    cluster_accuracy = []\n",
    "    \n",
    "    # Calculate accuracy for each cluster\n",
    "    for cluster_idx in range(k):\n",
    "        # Get distances for current cluster\n",
    "        cluster_distances = distances[:, cluster_idx]\n",
    "        \n",
    "        # Sort distances and get indices\n",
    "        sorted_indices = np.argsort(cluster_distances)\n",
    "        \n",
    "        # Number of points in the current cluster\n",
    "        num_points_in_cluster = np.sum(cluster_labels == cluster_idx)\n",
    "        \n",
    "        # Select top num_points_in_cluster distances\n",
    "        nearest_distances_indices = sorted_indices[:num_points_in_cluster]\n",
    "        \n",
    "        # Count correct labels (same as cluster label)\n",
    "        correct_labels_count = np.sum(cluster_labels[nearest_distances_indices] == cluster_idx)\n",
    "        \n",
    "        # Calculate accuracy for the current cluster\n",
    "        cluster_accuracy.append(correct_labels_count / num_points_in_cluster)\n",
    "    \n",
    "    # Calculate overall accuracy (mean of cluster accuracies)\n",
    "    overall_accuracy_M2_kmeans = np.mean(cluster_accuracy)\n",
    "    accuracy_results.append(overall_accuracy_M2_kmeans)\n",
    "    print(\"Accuracy Results:\",overall_accuracy_M2_kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "spectral = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)\n",
    "y_spectral = spectral.fit_predict(data_pca2)\n",
    "\n",
    "plt.scatter(data_pca2[y_spectral == 0, 0], data_pca2[y_spectral == 0, 1], s=100, c='red', label='Cluster 1')\n",
    "plt.scatter(data_pca2[y_spectral == 1, 0], data_pca2[y_spectral == 1, 1], s=100, c='blue', label='Cluster 2')\n",
    "plt.scatter(data_pca2[y_spectral == 2, 0], data_pca2[y_spectral == 2, 1], s=100, c='green', label='Cluster 3')\n",
    "plt.title('Clusters of measurements')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "s2_spectral = silhouette_score(data_pca2, y_spectral)\n",
    "print('Silhouette Score for Spectral clustering:', s2_spectral)\n",
    "\n",
    "# Checking accuracy for Spectral Clustering\n",
    "k = 3\n",
    "spectral = SpectralClustering(n_clusters=k, affinity='nearest_neighbors', random_state=42)\n",
    "cluster_labels_spectral = spectral.fit_predict(data_pca2)\n",
    "\n",
    "distances_spectral = pairwise_distances(data_pca2, cluster_centers, metric='euclidean')\n",
    "cluster_accuracy_spectral = []\n",
    "\n",
    "for cluster_idx in range(k):\n",
    "    cluster_distances_spectral = distances_spectral[:, cluster_idx]\n",
    "    sorted_indices_spectral = np.argsort(cluster_distances_spectral)\n",
    "    num_points_in_cluster_spectral = np.sum(cluster_labels_spectral == cluster_idx)\n",
    "    nearest_distances_indices_spectral = sorted_indices_spectral[:num_points_in_cluster_spectral]\n",
    "    correct_labels_count_spectral = np.sum(cluster_labels_spectral[nearest_distances_indices_spectral] == cluster_idx)\n",
    "    cluster_accuracy_spectral.append(correct_labels_count_spectral / num_points_in_cluster_spectral)\n",
    "\n",
    "overall_accuracy_M2_spectral = np.mean(cluster_accuracy_spectral)\n",
    "print(\"Accuracy Results for Spectral Clustering:\", overall_accuracy_M2_spectral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'> Hierarchy Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram = sch.dendrogram(sch.linkage(data_pca2, method = 'ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import numpy as np\n",
    "\n",
    "# Compute the hierarchical clustering\n",
    "Z = linkage(data_pca2, method='ward')\n",
    "\n",
    "# Determine cluster labels\n",
    "n_clusters = 4\n",
    "y_hc = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "\n",
    "print(\"Cluster labels:\", y_hc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check unique cluster labels\n",
    "unique_labels = np.unique(y_hc)\n",
    "print(\"Unique cluster labels:\", unique_labels)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in unique_labels:\n",
    "    plt.scatter(data_pca2[y_hc == label, 0], data_pca2[y_hc == label, 1], label=f'Cluster {label}')\n",
    "\n",
    "plt.title('Clusters of measurements')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce63a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame mapping encoded values to country names\n",
    "country_cluster_df = pd.DataFrame({'Country': country_names, 'Encoded_Value': encoded_values})\n",
    "\n",
    "# Merge the country_cluster_df with the cluster labels\n",
    "country_cluster_df['Cluster'] = pd.Series(y_hc)\n",
    "\n",
    "# Display the DataFrame with country names, encoded values, and cluster labels\n",
    "print(country_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy check\n",
    "s2_hierarchy = silhouette_score(data_pca2,y_hc)\n",
    "print('Silhouette Score for Hierarchy clustring :',s2_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28769aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Perform Hierarchical Clustering\n",
    "n_clusters = 4\n",
    "hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hc_labels = hc.fit_predict(data_pca2)\n",
    "\n",
    "# Calculate cluster centers\n",
    "cluster_centers = np.array([data_pca2[hc_labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "\n",
    "# Check cluster_centers array shape and contents\n",
    "print(f\"Cluster centers for n_clusters={n_clusters}:\")\n",
    "print(cluster_centers)\n",
    "print(f\"Shape: {cluster_centers.shape}\")\n",
    "\n",
    "# Calculate pairwise distances from each point to its cluster centroid\n",
    "distances = pairwise_distances(data_pca2, cluster_centers, metric='euclidean')\n",
    "\n",
    "cluster_accuracy = []\n",
    "\n",
    "# Calculate accuracy for each cluster\n",
    "for cluster_idx in range(n_clusters):\n",
    "    # Get distances for current cluster\n",
    "    cluster_distances = distances[:, cluster_idx]\n",
    "    \n",
    "    # Sort distances and get indices\n",
    "    sorted_indices = np.argsort(cluster_distances)\n",
    "    \n",
    "    # Number of points in the current cluster\n",
    "    num_points_in_cluster = np.sum(hc_labels == cluster_idx)\n",
    "    \n",
    "    # Select top num_points_in_cluster distances\n",
    "    nearest_distances_indices = sorted_indices[:num_points_in_cluster]\n",
    "    \n",
    "    # Count correct labels (same as cluster label)\n",
    "    correct_labels_count = np.sum(hc_labels[nearest_distances_indices] == cluster_idx)\n",
    "    \n",
    "    # Calculate accuracy for the current cluster\n",
    "    cluster_accuracy.append(correct_labels_count / num_points_in_cluster)\n",
    "\n",
    "# Calculate overall accuracy (mean of cluster accuracies)\n",
    "overall_accuracy_M2_hierarchial = np.mean(cluster_accuracy)\n",
    "print(f\"Accuracy for Hierarchical Clustering with n_clusters={n_clusters}: {overall_accuracy_M2_hierarchial:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='purple'> DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.2,min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(data_pca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_pca2[:,0],data_pca2[:,1],c=dbscan_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c65e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame mapping encoded values to country names\n",
    "country_cluster_df = pd.DataFrame({'Country': country_names, 'Encoded_Value': encoded_values})\n",
    "\n",
    "# Merge the country_cluster_df with the cluster labels\n",
    "country_cluster_df['Cluster'] = pd.Series(dbscan_labels)\n",
    "\n",
    "# Display the DataFrame with country names, encoded values, and cluster labels\n",
    "print(country_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['cluster']=dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c08a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "eps = 0.2\n",
    "min_samples = 5\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan_labels = dbscan.fit_predict(data_pca2)\n",
    "\n",
    "# Extract unique cluster labels (excluding noise)\n",
    "\n",
    "# Calculate cluster centers\n",
    "cluster_centers = np.array([data_pca2[dbscan_labels == cluster].mean(axis=0) for cluster in dbscan_labels])\n",
    "\n",
    "# Check cluster_centers array shape and contents\n",
    "print(f\"Cluster centers for DBSCAN (excluding noise):\")\n",
    "print(cluster_centers)\n",
    "print(f\"Shape: {cluster_centers.shape}\")\n",
    "\n",
    "# Calculate pairwise distances from each point to its cluster centroid\n",
    "distances = pairwise_distances(data_pca2, cluster_centers, metric='euclidean')\n",
    "\n",
    "cluster_accuracy = []\n",
    "\n",
    "# Calculate accuracy for each cluster\n",
    "for cluster_idx, cluster_label in enumerate(dbscan_labels):\n",
    "    # Get distances for current cluster\n",
    "    cluster_distances = distances[:, cluster_idx]\n",
    "    \n",
    "    # Sort distances and get indices\n",
    "    sorted_indices = np.argsort(cluster_distances)\n",
    "    \n",
    "    # Number of points in the current cluster\n",
    "    num_points_in_cluster = np.sum(dbscan_labels == cluster_label)\n",
    "    \n",
    "    # Select top num_points_in_cluster distances\n",
    "    nearest_distances_indices = sorted_indices[:num_points_in_cluster]\n",
    "    \n",
    "    # Count correct labels (same as cluster label)\n",
    "    correct_labels_count = np.sum(dbscan_labels[nearest_distances_indices] == cluster_label)\n",
    "    \n",
    "    # Calculate accuracy for the current cluster\n",
    "    cluster_accuracy.append(correct_labels_count / num_points_in_cluster)\n",
    "\n",
    "# Calculate overall accuracy (mean of cluster accuracies)\n",
    "overall_accuracy_M2_DBSCAN = np.mean(cluster_accuracy)\n",
    "print(f\"Accuracy for DBSCAN with eps={eps}, min_samples={min_samples}: {overall_accuracy_M2_DBSCAN:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[data2['cluster']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_dbscan = silhouette_score(data_pca2, dbscan_labels)\n",
    "print(\"Silhouette Score for DBSCAN is:\", s2_dbscan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>Method 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize t-SNE with default parameters\n",
    "tsne = TSNE()\n",
    "\n",
    "# Fit and transform the data to 2 dimensions\n",
    "data_tsne = tsne.fit_transform(data3)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(data_tsne[:, 0], data_tsne[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating clusters\n",
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(data_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'> K-means Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters\n",
    "plt.scatter(data_tsne[y_kmeans == 0, 0], data_tsne[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(data_tsne[y_kmeans == 1, 0], data_tsne[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(data_tsne[y_kmeans == 2, 0], data_tsne[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "# Add labels and legend\n",
    "plt.xlabel('TSNE 1')\n",
    "plt.ylabel('TSNE 2')\n",
    "plt.title('KMeans Clustering')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468be889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame mapping encoded values to country names\n",
    "country_cluster_df = pd.DataFrame({'Country': country_names, 'Encoded_Value': encoded_values})\n",
    "\n",
    "# Merge the country_cluster_df with the cluster labels\n",
    "country_cluster_df['Cluster'] = pd.Series(y_kmeans)\n",
    "\n",
    "# Display the DataFrame with country names, encoded values, and cluster labels\n",
    "print(country_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy check\n",
    "s3_kmeans = silhouette_score(data_tsne, y_kmeans)\n",
    "print('Silhouette Score for K-means clustring :', s3_kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "k_values = [ 3]\n",
    "accuracy_results = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(data_tsne)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Check cluster_centers array shape and contents\n",
    "    print(f\"Cluster centers for k={k}:\")\n",
    "    print(cluster_centers)\n",
    "    print(f\"Shape: {cluster_centers.shape}\")\n",
    "    \n",
    "    # Calculate pairwise distances from each point to its cluster centroid\n",
    "    distances = pairwise_distances(data_tsne, cluster_centers, metric='euclidean')\n",
    "    \n",
    "    cluster_accuracy = []\n",
    "    \n",
    "    # Calculate accuracy for each cluster\n",
    "    for cluster_idx in range(k):\n",
    "        # Get distances for current cluster\n",
    "        cluster_distances = distances[:, cluster_idx]\n",
    "        \n",
    "        # Sort distances and get indices\n",
    "        sorted_indices = np.argsort(cluster_distances)\n",
    "        \n",
    "        # Number of points in the current cluster\n",
    "        num_points_in_cluster = np.sum(cluster_labels == cluster_idx)\n",
    "        \n",
    "        # Select top num_points_in_cluster distances\n",
    "        nearest_distances_indices = sorted_indices[:num_points_in_cluster]\n",
    "        \n",
    "        # Count correct labels (same as cluster label)\n",
    "        correct_labels_count = np.sum(cluster_labels[nearest_distances_indices] == cluster_idx)\n",
    "        \n",
    "        # Calculate accuracy for the current cluster\n",
    "        cluster_accuracy.append(correct_labels_count / num_points_in_cluster)\n",
    "    \n",
    "    # Calculate overall accuracy (mean of cluster accuracies)\n",
    "    overall_accuracy_M3_kmeans = np.mean(cluster_accuracy)\n",
    "    accuracy_results.append(overall_accuracy_M3_kmeans)\n",
    "    print(\"Accuracy Results:\",overall_accuracy_M3_kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcd012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "spectral = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)\n",
    "y_spectral = spectral.fit_predict(data_tsne)\n",
    "\n",
    "plt.scatter(data_tsne[y_spectral == 0, 0], data_tsne[y_spectral == 0, 1], s=100, c='red', label='Cluster 1')\n",
    "plt.scatter(data_tsne[y_spectral == 1, 0], data_tsne[y_spectral == 1, 1], s=100, c='blue', label='Cluster 2')\n",
    "plt.scatter(data_tsne[y_spectral == 2, 0], data_tsne[y_spectral == 2, 1], s=100, c='green', label='Cluster 3')\n",
    "plt.title('Clusters of measurements')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "s3_spectral = silhouette_score(data_tsne, y_spectral)\n",
    "print('Silhouette Score for Spectral clustering:', s3_spectral)\n",
    "\n",
    "# Checking accuracy for Spectral Clustering\n",
    "k = 3\n",
    "spectral = SpectralClustering(n_clusters=k, affinity='nearest_neighbors', random_state=42)\n",
    "cluster_labels_spectral = spectral.fit_predict(data_tsne)\n",
    "\n",
    "distances_spectral = pairwise_distances(data_tsne, cluster_centers, metric='euclidean')\n",
    "cluster_accuracy_spectral = []\n",
    "\n",
    "for cluster_idx in range(k):\n",
    "    cluster_distances_spectral = distances_spectral[:, cluster_idx]\n",
    "    sorted_indices_spectral = np.argsort(cluster_distances_spectral)\n",
    "    num_points_in_cluster_spectral = np.sum(cluster_labels_spectral == cluster_idx)\n",
    "    nearest_distances_indices_spectral = sorted_indices_spectral[:num_points_in_cluster_spectral]\n",
    "    correct_labels_count_spectral = np.sum(cluster_labels_spectral[nearest_distances_indices_spectral] == cluster_idx)\n",
    "    cluster_accuracy_spectral.append(correct_labels_count_spectral / num_points_in_cluster_spectral)\n",
    "\n",
    "overall_accuracy_M3_spectral = np.mean(cluster_accuracy_spectral)\n",
    "print(\"Accuracy Results for Spectral Clustering:\", overall_accuracy_M3_spectral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'> Hierarchy Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram = sch.dendrogram(sch.linkage(data_tsne, method = 'ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Compute the hierarchical clustering and determine cluster labels\n",
    "n_clusters = 4\n",
    "y_hc = fcluster(linkage(data_tsne, method='ward'), n_clusters, criterion='maxclust')\n",
    "\n",
    "print(\"Cluster labels:\", y_hc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check unique cluster labels\n",
    "unique_labels = np.unique(y_hc)\n",
    "print(\"Unique cluster labels:\", unique_labels)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in unique_labels:\n",
    "    plt.scatter(data_tsne[y_hc == label, 0], data_tsne[y_hc == label, 1], label=f'Cluster {label}')\n",
    "\n",
    "plt.title('Clusters of measurements')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame mapping encoded values to country names\n",
    "country_cluster_df = pd.DataFrame({'Country': country_names, 'Encoded_Value': encoded_values})\n",
    "\n",
    "# Merge the country_cluster_df with the cluster labels\n",
    "country_cluster_df['Cluster'] = pd.Series(y_hc)\n",
    "\n",
    "# Display the DataFrame with country names, encoded values, and cluster labels\n",
    "print(country_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy check\n",
    "s3_hierarchy = silhouette_score(data_tsne,y_hc)\n",
    "print('Silhouette Score for Hierarchy clustring :',s3_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Perform Hierarchical Clustering\n",
    "n_clusters = 4\n",
    "hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hc_labels = hc.fit_predict(data_tsne)\n",
    "\n",
    "# Calculate cluster centers\n",
    "cluster_centers = np.array([data_tsne[hc_labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "\n",
    "# Check cluster_centers array shape and contents\n",
    "print(f\"Cluster centers for n_clusters={n_clusters}:\")\n",
    "print(cluster_centers)\n",
    "print(f\"Shape: {cluster_centers.shape}\")\n",
    "\n",
    "# Calculate pairwise distances from each point to its cluster centroid\n",
    "distances = pairwise_distances(data_tsne, cluster_centers, metric='euclidean')\n",
    "\n",
    "cluster_accuracy = []\n",
    "\n",
    "# Calculate accuracy for each cluster\n",
    "for cluster_idx in range(n_clusters):\n",
    "    # Get distances for current cluster\n",
    "    cluster_distances = distances[:, cluster_idx]\n",
    "    \n",
    "    # Sort distances and get indices\n",
    "    sorted_indices = np.argsort(cluster_distances)\n",
    "    \n",
    "    # Number of points in the current cluster\n",
    "    num_points_in_cluster = np.sum(hc_labels == cluster_idx)\n",
    "    \n",
    "    # Select top num_points_in_cluster distances\n",
    "    nearest_distances_indices = sorted_indices[:num_points_in_cluster]\n",
    "    \n",
    "    # Count correct labels (same as cluster label)\n",
    "    correct_labels_count = np.sum(hc_labels[nearest_distances_indices] == cluster_idx)\n",
    "    \n",
    "    # Calculate accuracy for the current cluster\n",
    "    cluster_accuracy.append(correct_labels_count / num_points_in_cluster)\n",
    "\n",
    "# Calculate overall accuracy (mean of cluster accuracies)\n",
    "overall_accuracy_M3_hierarchial = np.mean(cluster_accuracy)\n",
    "print(f\"Accuracy for Hierarchical Clustering with n_clusters={n_clusters}: {overall_accuracy_M3_hierarchial:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dddeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.2,min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(data_tsne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b363446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define a colormap\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "\n",
    "# Count the number of points in each cluster\n",
    "unique_labels, counts = np.unique(dbscan_labels, return_counts=True)\n",
    "\n",
    "# Find the cluster with the maximum number of points\n",
    "max_count_label = unique_labels[np.argmax(counts)]\n",
    "\n",
    "# Scatter plot with adjusted transparency\n",
    "for label in unique_labels:\n",
    "    if label == max_count_label:\n",
    "        alpha = 0.3  # Light color for the cluster with the maximum number of points\n",
    "    else:\n",
    "        alpha = 1.0  # Dark color for remaining clusters\n",
    "    \n",
    "    # Get the color for the current cluster label from the colormap\n",
    "    color = cmap(label / len(unique_labels))\n",
    "    \n",
    "    # Plot points belonging to the current cluster with the specified color and transparency\n",
    "    plt.scatter(data_tsne[dbscan_labels == label, 0], data_tsne[dbscan_labels == label, 1], color=color, alpha=alpha)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dddedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame mapping encoded values to country names\n",
    "country_cluster_df = pd.DataFrame({'Country': country_names, 'Encoded_Value': encoded_values})\n",
    "\n",
    "# Merge the country_cluster_df with the cluster labels\n",
    "country_cluster_df['Cluster'] = pd.Series(dbscan_labels)\n",
    "\n",
    "# Display the DataFrame with country names, encoded values, and cluster labels\n",
    "print(country_cluster_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3['cluster']=dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "eps = 0.2\n",
    "min_samples = 5\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan_labels = dbscan.fit_predict(data_tsne)\n",
    "\n",
    "# Extract unique cluster labels (excluding noise)\n",
    "\n",
    "# Calculate cluster centers\n",
    "cluster_centers = np.array([data_tsne[dbscan_labels == cluster].mean(axis=0) for cluster in dbscan_labels])\n",
    "\n",
    "# Check cluster_centers array shape and contents\n",
    "print(f\"Cluster centers for DBSCAN (excluding noise):\")\n",
    "print(cluster_centers)\n",
    "print(f\"Shape: {cluster_centers.shape}\")\n",
    "\n",
    "# Calculate pairwise distances from each point to its cluster centroid\n",
    "distances = pairwise_distances(data_tsne, cluster_centers, metric='euclidean')\n",
    "\n",
    "cluster_accuracy = []\n",
    "\n",
    "# Calculate accuracy for each cluster\n",
    "for cluster_idx, cluster_label in enumerate(dbscan_labels):\n",
    "    # Get distances for current cluster\n",
    "    cluster_distances = distances[:, cluster_idx]\n",
    "    \n",
    "    # Sort distances and get indices\n",
    "    sorted_indices = np.argsort(cluster_distances)\n",
    "    \n",
    "    # Number of points in the current cluster\n",
    "    num_points_in_cluster = np.sum(dbscan_labels == cluster_label)\n",
    "    \n",
    "    # Select top num_points_in_cluster distances\n",
    "    nearest_distances_indices = sorted_indices[:num_points_in_cluster]\n",
    "    \n",
    "    # Count correct labels (same as cluster label)\n",
    "    correct_labels_count = np.sum(dbscan_labels[nearest_distances_indices] == cluster_label)\n",
    "    \n",
    "    # Calculate accuracy for the current cluster\n",
    "    cluster_accuracy.append(correct_labels_count / num_points_in_cluster)\n",
    "\n",
    "# Calculate overall accuracy (mean of cluster accuracies)\n",
    "overall_accuracy_M3_DBSCAN = np.mean(cluster_accuracy)\n",
    "print(f\"Accuracy for DBSCAN with eps={eps}, min_samples={min_samples}: {overall_accuracy_M3_DBSCAN:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3[data3['cluster']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348596ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_dbscan = silhouette_score(data_tsne, dbscan_labels)\n",
    "print(\"Silhouette Score for DBSCAN is:\", s3_dbscan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Method':['M1 kmeans','M1 Spectral', 'M1 hierarchy', 'M1 DBSCAN', 'M2 kmeans','M2 Spectral', 'M2 hierarchy', 'M2 DBSCAN', 'M3 kmeans','M3 Spectral', 'M3 hierarchy', 'M3 DBSCAN'],\n",
    "                   'Silhouette Score':[s1_kmeans,s1_spectral,s1_hierarchy,s1_dbscan,s2_kmeans,s2_spectral,s2_hierarchy,s2_dbscan,s3_kmeans,s3_spectral,s3_hierarchy,s3_dbscan]})\n",
    "# Style the DataFrame\n",
    "styled_df = df.style.set_properties(**{\n",
    "    'background-color': 'white',\n",
    "    'color': 'black',\n",
    "    'border': '1px solid black'\n",
    "})\n",
    "\n",
    "# Additional CSS to ensure the whole background is white\n",
    "styled_df = styled_df.set_table_styles(\n",
    "    [{\n",
    "        'selector': 'th',\n",
    "        'props': [('background-color', 'white'), ('color', 'black'), ('border', '1px solid black')]\n",
    "    }, {\n",
    "        'selector': 'td',\n",
    "        'props': [('background-color', 'white'), ('color', 'black'), ('border', '1px solid black')]\n",
    "    }, {\n",
    "        'selector': 'tbody',\n",
    "        'props': [('background-color', 'white')]\n",
    "    }, {\n",
    "        'selector': 'table',\n",
    "        'props': [('background-color', 'white')]\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Method': ['M1 kmeans','M1 Spectral', 'M1 hierarchy', 'M1 DBSCAN', 'M2 kmeans','M2 Spectral', 'M2 hierarchy', 'M2 DBSCAN', 'M3 kmeans','M3 Spectral', 'M3 hierarchy', 'M3 DBSCAN'],\n",
    "    'Accuracy(cluster compactness)': [\n",
    "        overall_accuracy_M1_kmeans,\n",
    "        overall_accuracy_M1_spectral,\n",
    "        overall_accuracy_M1_hierarchial,\n",
    "        overall_accuracy_M1_DBSCAN,\n",
    "        overall_accuracy_M2_kmeans,\n",
    "        overall_accuracy_M2_spectral,\n",
    "        overall_accuracy_M2_hierarchial,\n",
    "        overall_accuracy_M2_DBSCAN,\n",
    "        overall_accuracy_M3_kmeans,\n",
    "        overall_accuracy_M3_spectral,\n",
    "        overall_accuracy_M3_hierarchial,\n",
    "        overall_accuracy_M3_DBSCAN\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Style the DataFrame\n",
    "styled_df = df.style.set_properties(**{\n",
    "    'background-color': 'white',\n",
    "    'color': 'black',\n",
    "    'border': '1px solid black'\n",
    "})\n",
    "\n",
    "# Additional CSS to ensure the whole background is white\n",
    "styled_df = styled_df.set_table_styles(\n",
    "    [{\n",
    "        'selector': 'th',\n",
    "        'props': [('background-color', 'white'), ('color', 'black'), ('border', '1px solid black')]\n",
    "    }, {\n",
    "        'selector': 'td',\n",
    "        'props': [('background-color', 'white'), ('color', 'black'), ('border', '1px solid black')]\n",
    "    }, {\n",
    "        'selector': 'tbody',\n",
    "        'props': [('background-color', 'white')]\n",
    "    }, {\n",
    "        'selector': 'table',\n",
    "        'props': [('background-color', 'white')]\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
